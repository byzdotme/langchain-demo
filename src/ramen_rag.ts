// å¯¼å…¥ LangChain çš„æ ¸å¿ƒæ¨¡å—å’Œç±»å‹
import { OllamaEmbeddings } from "@langchain/ollama";
import { ChatOpenAI } from "@langchain/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { Document } from "langchain/document"; // å¯¼å…¥ Document ç±»å‹

// --- é…ç½®ä¿¡æ¯ ---
const RAMEN_REVIEWS_FILE: string = "./ramen_reviews.txt"; // æ–‡æ¡£è·¯å¾„
const OLLAMA_BASE_URL: string = "http://localhost:11434"; // Ollama æœåŠ¡åœ°å€
const CHAT_MODEL: string = "qwen:7b"; // ç”¨äºèŠå¤©çš„æ¨¡å‹
const EMBEDDINGS_MODEL: string = "nomic-embed-text"; // ç”¨äºç”ŸæˆåµŒå…¥çš„å‘é‡æ¨¡å‹

/**
 * ä¸»å‡½æ•°ï¼Œè¿è¡Œæ•´ä¸ª RAG æµç¨‹
 */
async function main(): Promise<void> {

  console.log("ğŸœ å¼€å§‹æ„å»ºæ‹‰é¢åº—é¡¾é—® RAG åº”ç”¨ (TypeScript ç‰ˆæœ¬)...");

  // 1. åŠ è½½æ–‡æ¡£ (Load)
  // ------------------------------------------
  console.log(`\n[æ­¥éª¤ 1] ä» '${RAMEN_REVIEWS_FILE}' åŠ è½½æ–‡æ¡£...`);
  const loader: TextLoader = new TextLoader(RAMEN_REVIEWS_FILE);
  const docs: Document[] = await loader.load();
  console.log(`  åŠ è½½äº† ${docs.length} ç¯‡æ–‡æ¡£ã€‚`);

  // 2. åˆ†å‰²æ–‡æ¡£ (Split)
  // ------------------------------------------
  console.log("\n[æ­¥éª¤ 2] å°†æ–‡æ¡£åˆ†å‰²æˆå°å—...");
  const splitter: RecursiveCharacterTextSplitter =
    new RecursiveCharacterTextSplitter({
      chunkSize: 500, // æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
      chunkOverlap: 50, // ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œä»¥ä¿è¯è¯­ä¹‰è¿ç»­æ€§
    });
  const splitDocs: Document[] = await splitter.splitDocuments(docs);
  console.log(`  æ–‡æ¡£è¢«åˆ†å‰²æˆ ${splitDocs.length} ä¸ªå°å—ã€‚`);

  // 3. åˆå§‹åŒ–æ¨¡å‹ (Initialize Models)
  // ------------------------------------------
  console.log("\n[æ­¥éª¤ 3] åˆå§‹åŒ– Ollama æ¨¡å‹...");
  const embeddings: OllamaEmbeddings = new OllamaEmbeddings({
    model: EMBEDDINGS_MODEL,
    baseUrl: OLLAMA_BASE_URL,
  });
  console.log(`  åµŒå…¥æ¨¡å‹: ${EMBEDDINGS_MODEL}`);

  const llm: ChatOpenAI = new ChatOpenAI({
    temperature: 0.7,
    model: "deepseek-chat",
    configuration: {
      baseURL: "https://api.deepseek.com",
      apiKey: process.env["DEEPSEEK_API_KEY"] || "",
    },
  });
  console.log(`  èŠå¤©æ¨¡å‹: ${CHAT_MODEL}`);

  // 4. åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨ (Embed & Store & Retrieve)
  // ------------------------------------------
  console.log("\n[æ­¥éª¤ 4] åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨...");
  const vectorstore: MemoryVectorStore = await MemoryVectorStore.fromDocuments(
    splitDocs,
    embeddings
  );
  console.log("  å†…å­˜å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸã€‚");

  const retriever = vectorstore.asRetriever({
    k: 3, // è®¾ç½®ä¸ºè¿”å›æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£å—
  });
  console.log("  æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸï¼Œå°†è¿”å›æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£å—ã€‚");

  // 5. æ„å»º RAG é“¾ (Chain)
  // ------------------------------------------
  console.log("\n[æ­¥éª¤ 5] æ„å»º RAG é“¾...");

  const promptTemplate: string = `
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹‰é¢åº—é¡¾é—®ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„â€œä¸Šä¸‹æ–‡ä¿¡æ¯â€ï¼Œç”¨ä¸­æ–‡ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
ä½ çš„å›ç­”å¿…é¡»å®Œå…¨åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ä»»ä½•å†…å®¹ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´â€œæ ¹æ®æˆ‘æ‰€çŸ¥çš„ä¿¡æ¯ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜â€ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

ç”¨æˆ·é—®é¢˜:
{question}

ä½ çš„å›ç­”:
`;
  const prompt = ChatPromptTemplate.fromTemplate(promptTemplate);

  // å®šä¹‰å¦‚ä½•å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶æ˜¾å¼å£°æ˜ç±»å‹
  const formatDocs = (docs: Document[]): string => {
    return docs
      .map((doc, i) => `--- æ–‡æ¡£ ${i + 1} ---\n${doc.pageContent}`)
      .join("\n\n");
  };

  // ä½¿ç”¨ LCEL æ„å»ºé“¾ï¼Œå¹¶ä¸ºé“¾æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºç±»å‹
  // RunnableSequence<string, string> è¡¨ç¤ºè¿™ä¸ªé“¾æ¥æ”¶ä¸€ä¸ªå­—ç¬¦ä¸²è¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²è¾“å‡º
  const chain: RunnableSequence<string, string> = RunnableSequence.from([
    {
      context: retriever.pipe(formatDocs),
      question: new RunnablePassthrough(),
    },
    prompt,
    llm,
    new StringOutputParser(),
  ]);
  console.log("  RAG é“¾æ„å»ºå®Œæˆï¼");

  // 6. æ‰§è¡Œé“¾å¹¶æé—® (Invoke)
  // ------------------------------------------
  console.log("\n[æ­¥éª¤ 6] æ‰§è¡Œé“¾å¹¶å¼€å§‹æé—®...");

  const questions: string[] = [
    "ä¸€ä¹æ‹‰é¢æœ‰æ²¡æœ‰ç´ é£Ÿé€‰é¡¹ï¼Ÿ",
    "å“ªå®¶åº—ä»¥æ²¾é¢å‡ºåï¼Œä»·æ ¼å¤§æ¦‚å¤šå°‘ï¼Ÿ",
    "æˆ‘æƒ³åƒç‚¹è¾£çš„ï¼Œæœ‰ä»€ä¹ˆæ¨èå—ï¼Ÿ",
    "ç§‹å¶åŸé™„è¿‘æœ‰ä»€ä¹ˆæ¨èçš„æ‹‰é¢åº—å—ï¼Ÿ",
    "é¢å±‹æ­¦è—çš„æ±¤åº•æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ",
  ];

  for (const question of questions) {
    console.log("\n==============================================");
    console.log(`ğŸ¤” æé—®: ${question}`);
    const result: string = await chain.invoke(question);
    console.log(`ğŸœ å›ç­”: ${result}`);
  }
  console.log("\n==============================================");
  console.log("\næ‰€æœ‰é—®é¢˜å›ç­”å®Œæ¯•ï¼");
}

// è¿è¡Œä¸»å‡½æ•°
main().catch(console.error);
